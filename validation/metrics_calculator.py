"""
metrics_calculator.py - Calcul des m√©triques de validation acad√©miques

Impl√©mente toutes les m√©triques standards pour l'√©valuation des mod√®les :
- M√©triques de r√©gression : MAPE, RMSE, MAE, R¬≤
- M√©triques de classification : Pr√©cision, Rappel, F1-Score
- M√©triques m√©tier : Taux de ruptures √©vit√©es, √©conomies
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
from sklearn.metrics import (
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    confusion_matrix,
    precision_score,
    recall_score,
    f1_score,
    accuracy_score
)
import logging
import json
from pathlib import Path

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MetricsCalculator:
    """
    Calculateur de m√©triques pour validation acad√©mique

    Fournit toutes les m√©triques standards n√©cessaires pour une √©valuation
    rigoureuse et acad√©miquement recevable des performances du syst√®me.
    """

    def __init__(self):
        """Initialise le calculateur de m√©triques"""
        self.metrics_history = []
        logger.info("Metrics Calculator initialis√©")

    def calculate_regression_metrics(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray,
        product_name: str = "Unknown"
    ) -> Dict[str, float]:
        """
        Calcule les m√©triques de r√©gression standards

        Args:
            y_true: Valeurs r√©elles
            y_pred: Valeurs pr√©dites
            product_name: Nom du produit (pour logging)

        Returns:
            Dictionnaire contenant MAPE, RMSE, MAE, R¬≤
        """
        # V√©rification des entr√©es
        if len(y_true) != len(y_pred):
            raise ValueError("Les arrays doivent avoir la m√™me longueur")

        if len(y_true) == 0:
            logger.warning(f"Pas de donn√©es pour {product_name}")
            return self._empty_regression_metrics()

        # Conversion en numpy arrays
        y_true = np.array(y_true)
        y_pred = np.array(y_pred)

        # Calcul des m√©triques
        metrics = {}

        # 1. MAPE (Mean Absolute Percentage Error)
        metrics['mape'] = self._calculate_mape(y_true, y_pred)

        # 2. RMSE (Root Mean Square Error)
        metrics['rmse'] = np.sqrt(mean_squared_error(y_true, y_pred))

        # 3. MAE (Mean Absolute Error)
        metrics['mae'] = mean_absolute_error(y_true, y_pred)

        # 4. R¬≤ Score
        # √âviter R¬≤ si variance nulle
        if np.var(y_true) > 0:
            metrics['r2_score'] = r2_score(y_true, y_pred)
        else:
            metrics['r2_score'] = 0.0

        # 5. M√©triques suppl√©mentaires utiles
        metrics['mean_error'] = np.mean(y_pred - y_true)  # Biais
        metrics['std_error'] = np.std(y_pred - y_true)  # √âcart-type des erreurs
        metrics['max_error'] = np.max(np.abs(y_pred - y_true))  # Erreur maximale

        # Arrondir toutes les m√©triques
        for key in metrics:
            metrics[key] = round(metrics[key], 3)

        logger.info(f"M√©triques r√©gression {product_name}: MAPE={metrics['mape']}%, RMSE={metrics['rmse']}")

        return metrics

    def calculate_classification_metrics(
        self,
        y_true: List[str],
        y_pred: List[str],
        labels: List[str] = None
    ) -> Dict[str, Any]:
        """
        Calcule les m√©triques de classification pour les alertes

        Args:
            y_true: Alertes r√©elles (Critique, Attention, OK)
            y_pred: Alertes pr√©dites
            labels: Labels des classes

        Returns:
            Dictionnaire avec pr√©cision, rappel, F1, matrice de confusion
        """
        if not y_true or not y_pred:
            return self._empty_classification_metrics()

        if labels is None:
            labels = ['Critique', 'Attention', 'OK']

        # Convertir en arrays numpy
        y_true = np.array(y_true)
        y_pred = np.array(y_pred)

        metrics = {}

        # 1. Accuracy globale
        metrics['accuracy'] = accuracy_score(y_true, y_pred)

        # 2. Pr√©cision par classe
        metrics['precision_per_class'] = {}
        for label in labels:
            # Cr√©er des labels binaires pour cette classe
            y_true_binary = (y_true == label).astype(int)
            y_pred_binary = (y_pred == label).astype(int)

            if np.sum(y_true_binary) > 0:  # S'il y a au moins un exemple de cette classe
                precision = precision_score(y_true_binary, y_pred_binary, zero_division=0)
                metrics['precision_per_class'][label] = round(precision, 3)
            else:
                metrics['precision_per_class'][label] = 0.0

        # 3. Rappel par classe
        metrics['recall_per_class'] = {}
        for label in labels:
            y_true_binary = (y_true == label).astype(int)
            y_pred_binary = (y_pred == label).astype(int)

            if np.sum(y_true_binary) > 0:
                recall = recall_score(y_true_binary, y_pred_binary, zero_division=0)
                metrics['recall_per_class'][label] = round(recall, 3)
            else:
                metrics['recall_per_class'][label] = 0.0

        # 4. F1-Score par classe
        metrics['f1_per_class'] = {}
        for label in labels:
            y_true_binary = (y_true == label).astype(int)
            y_pred_binary = (y_pred == label).astype(int)

            if np.sum(y_true_binary) > 0:
                f1 = f1_score(y_true_binary, y_pred_binary, zero_division=0)
                metrics['f1_per_class'][label] = round(f1, 3)
            else:
                metrics['f1_per_class'][label] = 0.0

        # 5. Matrice de confusion
        cm = confusion_matrix(y_true, y_pred, labels=labels)
        metrics['confusion_matrix'] = cm.tolist()
        metrics['confusion_matrix_labels'] = labels

        # 6. Moyennes pond√©r√©es
        metrics['precision_weighted'] = round(
            precision_score(y_true, y_pred, average='weighted', zero_division=0), 3
        )
        metrics['recall_weighted'] = round(
            recall_score(y_true, y_pred, average='weighted', zero_division=0), 3
        )
        metrics['f1_weighted'] = round(
            f1_score(y_true, y_pred, average='weighted', zero_division=0), 3
        )

        logger.info(f"M√©triques classification: Accuracy={metrics['accuracy']:.3f}, "
                   f"F1-weighted={metrics['f1_weighted']:.3f}")

        return metrics

    def calculate_business_metrics(
        self,
        predictions_df: pd.DataFrame,
        stock_level: float = 100,
        unit_price: float = 10.0,
        holding_cost_rate: float = 0.2
    ) -> Dict[str, float]:
        """
        Calcule les m√©triques m√©tier sp√©cifiques √† la gestion de stock

        Args:
            predictions_df: DataFrame avec colonnes [predicted, actual]
            stock_level: Niveau de stock initial
            unit_price: Prix unitaire du produit
            holding_cost_rate: Taux de co√ªt de stockage annuel

        Returns:
            M√©triques m√©tier (ruptures √©vit√©es, √©conomies, etc.)
        """
        metrics = {}

        if predictions_df.empty:
            return self._empty_business_metrics()

        # 1. Taux de ruptures √©vit√©es
        # Rupture = quand la demande r√©elle > stock disponible
        ruptures_sans_prediction = 0
        ruptures_avec_prediction = 0

        current_stock_sans = stock_level
        current_stock_avec = stock_level

        for _, row in predictions_df.iterrows():
            actual_demand = row.get('actual', 0)
            predicted_demand = row.get('predicted', 0)

            # Sans pr√©diction (on ne r√©approvisionne pas)
            if current_stock_sans < actual_demand:
                ruptures_sans_prediction += 1
            current_stock_sans = max(0, current_stock_sans - actual_demand)

            # Avec pr√©diction (on r√©approvisionne selon la pr√©diction)
            if predicted_demand > current_stock_avec:
                # R√©approvisionnement
                current_stock_avec += predicted_demand

            if current_stock_avec < actual_demand:
                ruptures_avec_prediction += 1
            current_stock_avec = max(0, current_stock_avec - actual_demand)

        total_days = len(predictions_df)
        metrics['ruptures_sans_prediction'] = ruptures_sans_prediction
        metrics['ruptures_avec_prediction'] = ruptures_avec_prediction

        if ruptures_sans_prediction > 0:
            metrics['taux_ruptures_evitees'] = round(
                (ruptures_sans_prediction - ruptures_avec_prediction) / ruptures_sans_prediction * 100, 2
            )
        else:
            metrics['taux_ruptures_evitees'] = 100.0

        # 2. Co√ªt des ruptures √©vit√©es
        ruptures_evitees = ruptures_sans_prediction - ruptures_avec_prediction
        cout_rupture_unitaire = unit_price * 2  # Hypoth√®se : co√ªt rupture = 2x prix unitaire
        metrics['economies_ruptures'] = round(ruptures_evitees * cout_rupture_unitaire, 2)

        # 3. Co√ªt de sur-stock
        overstock = predictions_df['predicted'].sum() - predictions_df['actual'].sum()
        if overstock > 0:
            # Co√ªt de stockage journalier
            daily_holding_cost = unit_price * holding_cost_rate / 365
            metrics['cout_surstock'] = round(overstock * daily_holding_cost, 2)
        else:
            metrics['cout_surstock'] = 0.0

        # 4. √âconomies nettes
        metrics['economies_nettes'] = round(
            metrics['economies_ruptures'] - metrics['cout_surstock'], 2
        )

        # 5. Taux de service (% jours sans rupture)
        metrics['taux_service'] = round(
            (total_days - ruptures_avec_prediction) / total_days * 100, 2
        )

        # 6. Niveau de stock moyen
        metrics['stock_moyen_predit'] = round(predictions_df['predicted'].mean(), 2)
        metrics['stock_moyen_reel'] = round(predictions_df['actual'].mean(), 2)

        logger.info(f"M√©triques m√©tier: Ruptures √©vit√©es={metrics['taux_ruptures_evitees']}%, "
                   f"Taux service={metrics['taux_service']}%")

        return metrics

    def calculate_all_metrics(
        self,
        results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Calcule toutes les m√©triques sur les r√©sultats de backtesting

        Args:
            results: R√©sultats du BacktestingEngine

        Returns:
            Dictionnaire complet avec toutes les m√©triques
        """
        all_metrics = {
            'global_metrics': {},
            'per_product_metrics': {},
            'summary': {}
        }

        # Collecter toutes les pr√©dictions
        all_predictions = []
        all_actuals = []

        for product_id, product_data in results.get('product_results', {}).items():
            if 'predictions' not in product_data:
                continue

            predictions_list = product_data['predictions']
            if not predictions_list:
                continue

            # Cr√©er DataFrame pour ce produit
            df = pd.DataFrame(predictions_list)

            if 'predicted' in df.columns and 'actual' in df.columns:
                # M√©triques de r√©gression pour ce produit
                regression_metrics = self.calculate_regression_metrics(
                    df['actual'].values,
                    df['predicted'].values,
                    product_data.get('name', f'Product {product_id}')
                )

                # M√©triques m√©tier pour ce produit
                business_metrics = self.calculate_business_metrics(df)

                all_metrics['per_product_metrics'][product_id] = {
                    'name': product_data.get('name'),
                    'regression': regression_metrics,
                    'business': business_metrics
                }

                # Ajouter aux listes globales
                all_predictions.extend(df['predicted'].tolist())
                all_actuals.extend(df['actual'].tolist())

        # M√©triques globales
        if all_predictions and all_actuals:
            all_metrics['global_metrics'] = self.calculate_regression_metrics(
                np.array(all_actuals),
                np.array(all_predictions),
                "Global"
            )

            # R√©sum√© ex√©cutif
            all_metrics['summary'] = self._create_summary(all_metrics)

        return all_metrics

    def _calculate_mape(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:
        """
        Calcule le MAPE (Mean Absolute Percentage Error)

        Args:
            y_true: Valeurs r√©elles
            y_pred: Valeurs pr√©dites

        Returns:
            MAPE en pourcentage
        """
        # √âviter la division par z√©ro
        mask = y_true != 0
        if not mask.any():
            return 100.0

        mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100
        return min(mape, 100.0)  # Limiter √† 100%

    def _empty_regression_metrics(self) -> Dict[str, float]:
        """Retourne des m√©triques vides pour la r√©gression"""
        return {
            'mape': 100.0,
            'rmse': 0.0,
            'mae': 0.0,
            'r2_score': 0.0,
            'mean_error': 0.0,
            'std_error': 0.0,
            'max_error': 0.0
        }

    def _empty_classification_metrics(self) -> Dict[str, Any]:
        """Retourne des m√©triques vides pour la classification"""
        return {
            'accuracy': 0.0,
            'precision_per_class': {},
            'recall_per_class': {},
            'f1_per_class': {},
            'confusion_matrix': [],
            'precision_weighted': 0.0,
            'recall_weighted': 0.0,
            'f1_weighted': 0.0
        }

    def _empty_business_metrics(self) -> Dict[str, float]:
        """Retourne des m√©triques m√©tier vides"""
        return {
            'taux_ruptures_evitees': 0.0,
            'economies_ruptures': 0.0,
            'cout_surstock': 0.0,
            'economies_nettes': 0.0,
            'taux_service': 0.0,
            'stock_moyen_predit': 0.0,
            'stock_moyen_reel': 0.0
        }

    def _create_summary(self, all_metrics: Dict) -> Dict[str, Any]:
        """
        Cr√©e un r√©sum√© ex√©cutif des m√©triques

        Args:
            all_metrics: Toutes les m√©triques calcul√©es

        Returns:
            R√©sum√© avec les indicateurs cl√©s
        """
        summary = {}

        # M√©triques globales de r√©gression
        if 'global_metrics' in all_metrics:
            global_m = all_metrics['global_metrics']
            summary['mape_global'] = global_m.get('mape', 100.0)
            summary['rmse_global'] = global_m.get('rmse', 0.0)
            summary['r2_global'] = global_m.get('r2_score', 0.0)

        # Moyennes des m√©triques m√©tier
        if 'per_product_metrics' in all_metrics:
            business_metrics = []
            for product_metrics in all_metrics['per_product_metrics'].values():
                if 'business' in product_metrics:
                    business_metrics.append(product_metrics['business'])

            if business_metrics:
                summary['taux_ruptures_evitees_moyen'] = round(
                    np.mean([m['taux_ruptures_evitees'] for m in business_metrics]), 2
                )
                summary['taux_service_moyen'] = round(
                    np.mean([m['taux_service'] for m in business_metrics]), 2
                )
                summary['economies_nettes_totales'] = round(
                    np.sum([m['economies_nettes'] for m in business_metrics]), 2
                )

        # Performance acad√©mique
        if summary.get('mape_global', 100) < 10:
            summary['performance_niveau'] = 'Excellent'
        elif summary.get('mape_global', 100) < 15:
            summary['performance_niveau'] = 'Bon'
        elif summary.get('mape_global', 100) < 25:
            summary['performance_niveau'] = 'Acceptable'
        else:
            summary['performance_niveau'] = '√Ä am√©liorer'

        # Validation acad√©mique
        summary['validation_academique'] = {
            'methodologie': 'Split temporel strict (2022-2023 train, 2024 test)',
            'metriques_utilisees': ['MAPE', 'RMSE', 'MAE', 'R¬≤', 'Precision', 'Recall', 'F1'],
            'reproductible': True,
            'sans_data_leakage': True
        }

        return summary

    def save_metrics(self, metrics: Dict, filename: str = "validation_metrics.json"):
        """
        Sauvegarde les m√©triques dans un fichier JSON

        Args:
            metrics: M√©triques √† sauvegarder
            filename: Nom du fichier de sortie
        """
        output_path = Path("validation") / filename

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(metrics, f, indent=2, ensure_ascii=False)

        logger.info(f"M√©triques sauvegard√©es dans {output_path}")


def main():
    """Test du calculateur de m√©triques"""
    calculator = MetricsCalculator()

    # Test avec des donn√©es synth√©tiques
    print("\nüìä Test du calculateur de m√©triques")

    # Test m√©triques de r√©gression
    y_true = np.array([100, 150, 200, 120, 180])
    y_pred = np.array([95, 160, 190, 125, 175])

    regression_metrics = calculator.calculate_regression_metrics(y_true, y_pred, "Test Product")
    print(f"\nM√©triques de r√©gression:")
    for key, value in regression_metrics.items():
        print(f"  {key}: {value}")

    # Test m√©triques de classification
    y_true_class = ['Critique', 'OK', 'Attention', 'OK', 'Critique']
    y_pred_class = ['Critique', 'OK', 'OK', 'OK', 'Attention']

    classification_metrics = calculator.calculate_classification_metrics(y_true_class, y_pred_class)
    print(f"\nM√©triques de classification:")
    print(f"  Accuracy: {classification_metrics['accuracy']}")
    print(f"  F1 weighted: {classification_metrics['f1_weighted']}")

    # Test m√©triques m√©tier
    df = pd.DataFrame({
        'predicted': y_pred,
        'actual': y_true
    })

    business_metrics = calculator.calculate_business_metrics(df)
    print(f"\nM√©triques m√©tier:")
    print(f"  Taux service: {business_metrics['taux_service']}%")
    print(f"  √âconomies nettes: {business_metrics['economies_nettes']}‚Ç¨")


if __name__ == "__main__":
    main()